{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoders: Binary Recurrent Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholder\n",
    "Theoretically it should be possible to create auto-encoders using recurrent layers. After researching this a bit and trying various combinations, turned out it was difficult to make something which came even remotely close to the performance of convolutional auto-encoders on this data. They only ever reached F1 score around 0.5, which is way less than the *near* 0.7 of the convolutional counterparts.\n",
    "\n",
    "Various approaches were tested;\n",
    "- Using the last output of an LSTM as latent vector and repeating it in the decoder.\n",
    "- Using all of the output as latent vector.\n",
    "- Using the hidden stated of the encoder as input to the decoder.\n",
    "- Build a Machine Translation *Seq2Seq* sort of decoder.\n",
    "\n",
    "But none of these worked all the well. Not sure if this is because of the data or the implementation or both. *We might revist this in the future*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Note on the data set \n",
    "The data set used here is not particularly complex and/or big. It's not really all that challenging to find the fraud. In an ideal world we'd be using more complex data sets to show the real power of Deep Learning. There are a bunch of PCA'ed data sets available, but the PCA obfuscates some of the elements that are useful. \n",
    "*These examples are meant to show the possibilities, it's not so useful to interpret their performance on this data set*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "import datetime as dt\n",
    "\n",
    "import d373c7.features as ft\n",
    "import d373c7.engines as en\n",
    "import d373c7.pytorch as pt\n",
    "import d373c7.pytorch.models as pm\n",
    "import d373c7.plot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set a random seed for Numpy and Torch\n",
    "> Will make sure we always sample in the same way. Makes it easier to compare results. At some point it should been removed to test the model stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Numpy\n",
    "np.random.seed(42)\n",
    "# Torch\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define base feature and read the File\n",
    "The base features are features found in the input file. They need to be defined after which the file can be read using the `EnginePandasNumpy`. Using the `from_csv` method.\n",
    "The `from_csv` method will read the file and return a Pandas DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Change this to read from another location\n",
    "file = '../../../../data/bs140513_032310.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 14:54:28.924 d373c7.engines.common          INFO     Start Engine...\n",
      "2021-12-29 14:54:28.924 d373c7.engines.panda_numpy     INFO     Pandas Version : 1.1.4\n",
      "2021-12-29 14:54:28.925 d373c7.engines.panda_numpy     INFO     Numpy Version : 1.19.2\n",
      "2021-12-29 14:54:28.925 d373c7.engines.panda_numpy     INFO     Building Panda for : InternalKeyTime from file ../../../../data/bs140513_032310.csv\n",
      "2021-12-29 14:54:29.137 d373c7.engines.panda_numpy     INFO     Building Panda for : <Source_Derive_Source> from DataFrame. Inference mode <False>\n",
      "2021-12-29 14:54:29.137 d373c7.engines.panda_numpy     INFO     Reshaping DataFrame to: Source_Derive_Source\n",
      "2021-12-29 14:54:29.143 d373c7.engines.panda_numpy     INFO     Done creating Source_Derive_Source. Shape=(594643, 7)\n",
      "2021-12-29 14:54:29.592 d373c7.engines.panda_numpy     INFO     Reshaping DataFrame to: InternalKeyTime\n",
      "2021-12-29 14:54:29.637 d373c7.engines.panda_numpy     INFO     Start creating stacked series for Target Tensor Definition <learning> using 8 process(es)\n",
      "2021-12-29 14:54:34.202 d373c7.engines.panda_numpy     INFO     Returning series of types ['int8'].\n",
      "2021-12-29 14:54:34.202 d373c7.engines.panda_numpy     INFO     Done creating learning. Shapes=[(594643, 5, 107)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series Shapes\n",
      "[(594643, 5, 107)]\n",
      "['int8']\n"
     ]
    }
   ],
   "source": [
    "step = ft.FeatureSource('step', ft.FEATURE_TYPE_INT_16) \n",
    "customer = ft.FeatureSource('customer', ft.FEATURE_TYPE_STRING)\n",
    "age = ft.FeatureSource('age', ft.FEATURE_TYPE_CATEGORICAL)\n",
    "gender = ft.FeatureSource('gender', ft.FEATURE_TYPE_CATEGORICAL)\n",
    "merchant = ft.FeatureSource('merchant', ft.FEATURE_TYPE_CATEGORICAL)\n",
    "category = ft.FeatureSource('category', ft.FEATURE_TYPE_CATEGORICAL)\n",
    "amount = ft.FeatureSource('amount', ft.FEATURE_TYPE_FLOAT)\n",
    "fraud = ft.FeatureSource('fraud', ft.FEATURE_TYPE_INT_8)\n",
    "\n",
    "\n",
    "# Function to calculate the date and time from the step\n",
    "def step_to_date(step_count: int):\n",
    "    return dt.datetime(2020, 1, 1) + dt.timedelta(days=int(step_count))\n",
    "\n",
    "# Derrived Features\n",
    "amount_binned = ft.FeatureBin('amount_bin', ft.FEATURE_TYPE_INT_16, amount, 30)\n",
    "date_time = ft.FeatureExpression('date', ft.FEATURE_TYPE_DATE_TIME, step_to_date, [step])\n",
    "\n",
    "amount_oh = ft.FeatureOneHot('amount_one_hot', ft.FEATURE_TYPE_INT_8, amount_binned)\n",
    "age_oh = ft.FeatureOneHot('age_one_hot', ft.FEATURE_TYPE_INT_8, age)\n",
    "gender_oh = ft.FeatureOneHot('gender_one_hot', ft.FEATURE_TYPE_INT_8, gender)\n",
    "merchant_oh = ft.FeatureOneHot('merchant_one_hot', ft.FEATURE_TYPE_INT_8, merchant)\n",
    "category_oh = ft.FeatureOneHot('category_one_hot', ft.FEATURE_TYPE_INT_8, category)\n",
    "fraud_label = ft.FeatureLabelBinary('fraud_label', ft.FEATURE_TYPE_INT_8, fraud)\n",
    "\n",
    "learning_features = ft.TensorDefinition(\n",
    "    'learning', \n",
    "    [\n",
    "        age_oh,\n",
    "        gender_oh,\n",
    "        merchant_oh,\n",
    "        category_oh,\n",
    "        amount_oh\n",
    "    ])\n",
    "\n",
    "with en.EnginePandasNumpy(num_threads=8) as e:\n",
    "    series_list = e.to_series_stacked(\n",
    "        learning_features, file, key_feature=customer, time_feature=date_time, window=5, inference=False\n",
    "    )\n",
    "\n",
    "print('Series Shapes')\n",
    "print(series_list.shapes)\n",
    "print(series_list.dtype_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle the data\n",
    "Time to split the data. For time series data it is very important to keep the order of the data. Below split will start from the end and work it's way to the front of the data. Doing so the training, validation and test data are nicely colocated in time. You almost *never* want to plain shuffle time based data.\n",
    "\n",
    "> 1. Split out a test-set of size `test_records`. This is used for model testing.\n",
    "> 2. Split out a validation-set of size `validation_records`. It will be used to monitor overfitting during training\n",
    "> 3. All the rest is considered training data.\n",
    "\n",
    "For time-series we'll perform an additional action.\n",
    "> 1. The series at the beginning of the data set will all be more or less empty as there is no history, that is not so useful during training, ideally we have records with history and complete series, sometimes named 'mature' series. We'll throw away the first couple of entries.\n",
    "\n",
    "__Important__; please make sure the data is ordered in ascending fashion on a date(time) field. The split function does not order the data, it assumes the data is in the correct order.\n",
    "\n",
    "For auto-encoders we perform a 5th step, all fraud records will be removed from the training and validation data. The auto-encoder will only see *non-fraud* records during training.\n",
    "> 1. Remove fraud records from training and validation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
